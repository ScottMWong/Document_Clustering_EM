{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e648973",
   "metadata": {},
   "source": [
    "### Expectation-Maximization for Clustering (Movie Reviews)\n",
    "In this notebook, I will demonstrate how the expectation-maximization (EM) algorithm can be applied to document clustering. The EM algorithm involves iterations with two steps, an expectation (E) step which creates an expectation of the log-likelihood using the current estimates for the parameters, and a maximization (M) step, which computes values for the parameters maximizing the expectation of the log-likelihood from the E step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b377f",
   "metadata": {},
   "source": [
    "In document clustering, the parameters to be optimised are the cluster proportions and the word proportions for each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b2380",
   "metadata": {},
   "source": [
    "For more detailed information about the mathematics behind the EM algorithm, consult further resources such as this Wikipedia article: https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftEM:\n",
    "    \n",
    "    def __init__(self, K, tau_max=100):\n",
    "        self.K = K               \n",
    "        self.tau_max = tau_max  \n",
    "\n",
    "        self.cluster_probs = None     # placeholder for mixing ratio\n",
    "        self.word_probs = None      # placeholder for cluster member effective counts\n",
    "\n",
    "    def fit(self, x):\n",
    "        n_samples = x.shape[0]\n",
    "        n_words = x.shape[1]\n",
    "        # initialization:\n",
    "        # start with equal probs for each cluster\n",
    "        self.cluster_probs = np.array([1/self.K] * self.K) \n",
    "        # Start with random weightings for the words\n",
    "        # Need to normalise so each row sums to 1\n",
    "        # Numpy has some weird defaults, need to covert to 2D array\n",
    "        self.word_probs = np.random.rand(self.K,n_words)\n",
    "        word_probs_norm_coeff = np.sum(self.word_probs,axis=1,keepdims=True)\n",
    "        self.word_probs = self.word_probs/np.array(word_probs_norm_coeff)\n",
    "        \n",
    "        terminate= False\n",
    "        tau = 1\n",
    "        # fitting loop - we iteratively take E and M steps until the termination criterion is met.\n",
    "        cluster_probs_old = self.cluster_probs\n",
    "        word_probs_old = self.word_probs\n",
    "        cluster_preds = np.zeros((n_samples,self.K))\n",
    "        while (not terminate):\n",
    "\n",
    "            # E step:\n",
    "            for n in range(0,n_samples):\n",
    "                for k in range(0,self.K):\n",
    "                    # calculate the log likelihood of cluster given doc based on the estimated cluster probs and word-cluster probs\n",
    "                    cluster_raw_word_preds = np.multiply(x[n], np.where(self.word_probs[k]>0,np.log(self.word_probs[k]),-100))\n",
    "                    cluster_word_preds_sum = np.sum(cluster_raw_word_preds)\n",
    "                    cluster_preds[n,k] = np.log(self.cluster_probs[k]) + cluster_word_preds_sum\n",
    "                # Normalise the total probs to 1 for each doc\n",
    "                # Also takes probs out of log space\n",
    "                cluster_preds[n] = np.exp(cluster_preds[n])\n",
    "                cluster_preds_norm_coeff = np.sum(cluster_preds[n])\n",
    "                cluster_preds[n] = cluster_preds[n]/cluster_preds_norm_coeff\n",
    "\n",
    "            # M step\n",
    "            # Cluster probs\n",
    "            self.cluster_probs = np.sum(cluster_preds,axis=0)/n_samples\n",
    "            # Word probs\n",
    "            for k in range(0,self.K):\n",
    "                cluster_word_freqs = np.sum((cluster_preds[:,k].reshape(n_samples,1)*x),axis=0)\n",
    "                cluster_word_freqs_norm_coeff = np.sum(cluster_word_freqs)\n",
    "                self.word_probs[k] = cluster_word_freqs/cluster_word_freqs_norm_coeff\n",
    "\n",
    "            # increase iteration counter\n",
    "            tau +=1\n",
    "            # check termination condition\n",
    "            terminate = tau == self.tau_max or (np.array_equal(cluster_probs_old, self.cluster_probs) and np.array_equal(word_probs_old, self.word_probs))\n",
    "            cluster_probs_old = self.cluster_probs\n",
    "            word_probs_old = self.word_probs\n",
    "        print(\"Finished fitting at iteration\", tau)\n",
    "        \n",
    "        \n",
    "    # In a clustering-context, `predict` is equivalent to obtaining cluster assignments for new data\n",
    "    def predict(self, x):\n",
    "        n_samples = x.shape[0]\n",
    "        n_words = x.shape[1]\n",
    "        cluster_preds = np.zeros((n_samples, self.K))\n",
    "        for n in range(0,n_samples):\n",
    "            for k in range(0,self.K):\n",
    "                # calculate the log likelihood of cluster given doc based on the estimated cluster probs and word-cluster probs\n",
    "                cluster_raw_word_preds = np.multiply(x[n], np.where(self.word_probs[k]>0,np.log(self.word_probs[k]),-100))\n",
    "                cluster_word_preds_sum = np.sum(cluster_raw_word_preds)\n",
    "                cluster_preds[n,k] = np.log(self.cluster_probs[k]) + cluster_word_preds_sum\n",
    "            # Normalise the total probs to 1 for each doc\n",
    "            # Also takes probs out of log space\n",
    "            cluster_preds[n] = np.exp(cluster_preds[n])\n",
    "            cluster_preds_norm_coeff = np.sum(cluster_preds[n])\n",
    "            cluster_preds[n] = cluster_preds[n]/cluster_preds_norm_coeff\n",
    "        return cluster_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardEM:\n",
    "    \n",
    "    def __init__(self, K, tau_max=100):\n",
    "        self.K = K               \n",
    "        self.tau_max = tau_max  \n",
    "\n",
    "        self.cluster_probs = None     # placeholder for mixing ratio\n",
    "        self.word_probs = None      # placeholder for cluster member effective counts\n",
    "\n",
    "    def fit(self, x):\n",
    "        n_samples = x.shape[0]\n",
    "        n_words = x.shape[1]\n",
    "        # initialization:\n",
    "        # start with equal probs for each cluster\n",
    "        self.cluster_probs = np.array([1/self.K] * self.K) \n",
    "        # Start with random weightings for the words\n",
    "        # Need to normalise so each row sums to 1\n",
    "        # Numpy has some weird defaults, need to covert to 2D array\n",
    "        self.word_probs = np.random.rand(self.K,n_words)\n",
    "        word_probs_norm_coeff = np.sum(self.word_probs,axis=1,keepdims=True)\n",
    "        self.word_probs = self.word_probs/np.array(word_probs_norm_coeff)\n",
    "        \n",
    "        terminate= False\n",
    "        tau = 1\n",
    "        # fitting loop - we iteratively take E and M steps until the termination criterion is met.\n",
    "        cluster_probs_old = self.cluster_probs\n",
    "        word_probs_old = self.word_probs\n",
    "        cluster_preds = np.zeros((n_samples,self.K))\n",
    "        while (not terminate):\n",
    "            # E step:\n",
    "            for n in range(0,n_samples):\n",
    "                for k in range(0,self.K):\n",
    "                    # calculate the log likelihood of cluster given doc based on the estimated cluster probs and word-cluster probs\n",
    "                    cluster_raw_word_preds = np.multiply(x[n], np.where(self.word_probs[k]>0,np.log(self.word_probs[k]),-10))\n",
    "                    cluster_word_preds_sum = np.sum(cluster_raw_word_preds)\n",
    "                    cluster_preds[n,k] = np.log(self.cluster_probs[k]) + cluster_word_preds_sum\n",
    "                # Make argmax of the preds 1, all the rest 0\n",
    "                cluster_preds[n] = np.exp(cluster_preds[n])\n",
    "                best_k = np.argmax(cluster_preds[n])\n",
    "                cluster_preds[n] = np.zeros(self.K)\n",
    "                cluster_preds[n,best_k] = 1\n",
    "\n",
    "            # M step\n",
    "            # Cluster probs\n",
    "            self.cluster_probs = np.sum(cluster_preds,axis=0)/n_samples\n",
    "            # Word probs\n",
    "            for k in range(0,self.K):\n",
    "                cluster_word_freqs = np.sum((cluster_preds[:,k].reshape(n_samples,1)*x),axis=0)\n",
    "                cluster_word_freqs_norm_coeff = np.sum(cluster_word_freqs)\n",
    "                self.word_probs[k] = cluster_word_freqs/cluster_word_freqs_norm_coeff\n",
    "\n",
    "\n",
    "            # increase iteration counter\n",
    "            tau +=1\n",
    "            # check termination condition\n",
    "            terminate = tau == self.tau_max or (np.array_equal(cluster_probs_old, self.cluster_probs) and np.array_equal(word_probs_old, self.word_probs))\n",
    "            cluster_probs_old = self.cluster_probs\n",
    "            word_probs_old = self.word_probs\n",
    "        print(\"Finished fitting at iteration\", tau)\n",
    "        \n",
    "        \n",
    "    # In a clustering-context, `predict` is equivalent to obtaining cluster assignments for new data\n",
    "    def predict(self, x):\n",
    "        n_samples = x.shape[0]\n",
    "        n_words = x.shape[1]\n",
    "        cluster_preds = np.zeros((n_samples, self.K))\n",
    "        for n in range(0,n_samples):\n",
    "            for k in range(0,self.K):\n",
    "                # calculate the log likelihood of cluster given doc based on the estimated cluster probs and word-cluster probs\n",
    "                cluster_raw_word_preds = np.multiply(x[n], np.where(self.word_probs[k]>0,np.log(self.word_probs[k]),-10))\n",
    "                cluster_word_preds_sum = np.sum(cluster_raw_word_preds)\n",
    "                cluster_preds[n,k] = np.log(self.cluster_probs[k]) + cluster_word_preds_sum\n",
    "            # Make argmax of the preds 1, all the rest 0\n",
    "            cluster_preds[n] = np.exp(cluster_preds[n])\n",
    "            best_k = np.argmax(cluster_preds[n])\n",
    "            cluster_preds[n] = np.zeros(self.K)\n",
    "            cluster_preds[n,best_k] = 1\n",
    "        return cluster_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a85e4a",
   "metadata": {},
   "source": [
    "The dataset we will use is a set of genre labelled IMDB reviews, sourced from Kaggle user radmirkaz at https://www.kaggle.com/datasets/hijest/genre-classification-dataset-imdb. I've converted the file into a tab-separated values file beforehand (from the original with \" ::: \" separators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d410ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labelled_IMDB_reviews.tsv', header=0, names=[\"index\",\"title\",\"genre\",\"description\"], sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a095d8a",
   "metadata": {},
   "source": [
    "Let's take a closer look at the contents of this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844eb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a56f8",
   "metadata": {},
   "source": [
    "We can see that two columns, corresponding to movie title and file index won't be useful for our EM algorithm. The genre column will be our labels for evaluation, while the description column contains our documents to apply the EM algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1536103",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.drop(labels=[\"index\",\"title\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4ab84",
   "metadata": {},
   "source": [
    "There are a lot of different genres of movie in our dataset. To simplify our task (and make later evaluation easier) we will select 5 genres of movie and discard the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "thriller, comedy, documentary, drama, horror"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
